---
title: "hw3"
author: "Lizbeth Gomez"
date: "10/10/2019"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, problem 1 }
library(tidyverse)
library(p8105.datasets)
data("instacart")

#1: Description:
nrow(instacart)
ncol(instacart)


#2:How many aisles are there, and which aisles are the most items ordered from?

instacart%>%
  count(aisle)
 

instacart %>%
  count(aisle, name = "aisle_count") %>% 
  arrange(desc(aisle_count))

#3: Make a plot that shows the number of items ordered in each aisle, limiting this to aisles with more than 10000 items ordered. Arrange aisles sensibly, and organize your plot so others can read it.

instacart %>%
  count(aisle, name = "items_count") %>% 
  arrange(desc (items_count)) %>% 
  filter(items_count > 10000) %>% 
ggplot(aes(x = items_count, y = aisle)) + 
  geom_point()

#4: Make a table showing the three most popular items in each of the aisles “baking ingredients”, “dog food care”, and “packaged vegetables fruits”. Include the number of times each item is ordered in your table.


instacart %>% 
  filter(aisle %in% c( "dog food care", "baking ingredients", "packaged vegetables fruits")) %>% 
  count(aisle, product_name) %>% 
  group_by(aisle) %>% 
  top_n(3) %>% 
  knitr::kable()


#5:Make a table showing the mean hour of the day at which Pink Lady Apples and Coffee Ice Cream are ordered on each day of the week; format this table for human readers (i.e. produce a 2 x 7 table)

instacart %>% 
  filter(product_name %in% c( "Pink Lady Apples", " Coffee Ice Cream")) %>% 
  group_by(order_dow) %>% 
  summarise(average_order_hour = mean(order_hour_of_day)) %>% 
  mutate(order_dow = recode(order_dow, `1` = 'Mon', `2` = 'Tues', `3` = 'Wed', `4` = 'Thur', `5` = 'Fri', `6` = 'Sat', `0` = 'Sun'))%>% 
  knitr::kable()


```


##Dataset description:
#The dataset contains 1,384,617 observations of 15 variables, where each row in the dataset is a product from an order. There is a single order per user in this dataset, which means that every single row is an item purchased at one point by the user at a given time. This dataset contains variables describing the time of day of item purchase( "order_hour_of_the_day"), the online order identifier for the user ("order_id"), whether this user had purchased the item at a previous time ("reordered"), etc. Illustrative example:For instance unser number 34, purchased organic whole milk, bananas, marinara pasta sauce, low fat yogurt and half and half. one thing is for sure, they're not allergic to dairy.

#Answers:
#1: There are 134 aisles in this dataset*
#2: Most products are purchased from the fresh fruit aisle and the packaged vegetable fruits aisle*
#3: see plot above*
#4: See table above*
#5: See table above*

```{r, problem 2}

 data("brfss_smart2010")

#Data cleaning
brfss_smart2010 = 
  brfss_smart2010%>%
  janitor::clean_names() %>% 
  rename( state = locationabbr,county = locationdesc)%>%
  filter(topic == "Overall Health") %>% 
  filter(response %in% c("Excellent", "Very good", "Good", "Fair", "Poor")) %>% 
  mutate(response =fct_reorder(response, display_order))

head(brfss_smart2010)


#In 2002, which states were observed at 7 or more locations? What about in 2010?

brfss_smart2010%>%
  filter(year ==2002) %>% 
  separate(county, into = c("state", "county"), sep = " - ") %>% 
  distinct(state, county) %>% 
  count(state) %>% 
  filter(n >= 7)


brfss_smart2010%>%
  filter(year ==2010) %>% 
  separate(county, into = c("state", "county"), sep = " - ") %>% 
  distinct(state, county) %>% 
  count(state) %>% 
  filter(n >= 7)  
  
#Construct a dataset that is limited to Excellent responses, and contains, year, state, and a variable that averages the data_value across locations within a state. 

excellent_data = 
  brfss_smart2010%>%
  filter(response %in% c("Excellent")) %>% 
  select(response, year, state, data_value) %>% 
  group_by(state) %>% 
  mutate (mean_data_value = mean(data_value))

#Make a “spaghetti” plot of this average value over time within a state (that is, make a plot showing a line for each state across years – the geom_line geometry and group aesthetic will help).
excellent_data%>%
  group_by (state) %>% 
ggplot(aes(x = year, y = mean_data_value, color = state)) +
  geom_point() +
  geom_line() + 
  labs (
    title = "Mean Data Values Over the Years",
        x = "Year",
        y = "Mean data values"
  )


#Make a two-panel plot showing, for the years 2006, and 2010, distribution of data_value for responses (“Poor” to “Excellent”) among locations in NY State
 ny_data =
    brfss_smart2010 %>%
    filter(state == "NY", year == "2006" | year == "2010") %>%
    arrange(year) 
 
 ny_data%>% 
   group_by(county) %>% 
   ggplot(aes(group = county, x = response, y = data_value, color = county)) + 
   geom_line() +
   facet_grid (~year) +
   labs(title = "Health Responses in New York for 2006 and 2010 by County", 
       x = "Health Response",
       y = "Data Value of Health Response") +
   scale_color_hue(name = "County in NY") +
   theme(legend.position = "bottom")


  
```

##Answers:
#1: in 2006 there were 6 states with seven or more locations observed, while in 2010 there were 14 
#2: see spaghetti plot above
#3: see plot above


```{r}

#This problem uses five weeks of accelerometer data collected on a 63 year-old male with BMI 25, who was admitted to the Advanced Cardiac Care Center of Columbia University Medical Center and diagnosed with congestive heart failure (CHF).

#Load, tidy, and otherwise wrangle the data. Your final dataset should include all originally observed variables and values; have useful variable names; include a weekday vs weekend variable; and encode data with reasonable variable classes. Describe the resulting dataset (e.g. what variables exist, how many observations, etc).

accel = 
  read.csv("./data/accel_data.csv") %>%
  janitor::clean_names() %>% 
  rename(day_number = day_id, day_of_the_week = day) %>% 
  rename_at(vars(starts_with("activity")), funs(str_replace(., "activity", "minute"))) %>% 
  mutate(weekday= 
           if_else (day_of_the_week == "Monday", "weekday",
           if_else (day_of_the_week == "Tuesday", "weekday", 
           if_else (day_of_the_week == "Wednesday", "weekday",
           if_else (day_of_the_week == "Thursday","weekday",  
           if_else (day_of_the_week == "Friday","weekday",
           if_else (day_of_the_week == "Saturday","weekend",
           if_else (day_of_the_week == "Sunday","weekend", NA_character_))))))))


#Using your tidied dataset, aggregate accross minutes to create a total activity variable for each day, and create a table showing these totals. Are any trends apparent?


  
# Make a single-panel plot that shows the 24-hour activity time courses for each day and use color to indicate day of the week. Describe in words any patterns or conclusions you can make based on this graph

```

#Description
##this dataset contains 35 observations with 1444 variables. This data represents almost four years of the patient's daily activity, down to each minute of a 24 hour day for 35 days.
